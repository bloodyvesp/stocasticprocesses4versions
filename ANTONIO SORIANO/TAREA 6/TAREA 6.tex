\documentclass[a5paper,oneside]{amsart}
\usepackage[scale={.9,.8}]{geometry}
\usepackage{mathrsfs}
\usepackage{dsfont}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{conjecture}{Conjecture}
\theoremstyle{definition}
\newtheorem{problema}{Problema}
%\newtheorem{problema}{Ejercicio}
\newtheorem*{definition}{Definition}
\newtheorem*{remark}{Remark}
\usepackage{enumitem}
\usepackage{listings}
\lstset{
language=R,
basicstyle=%\scriptsize
\ttfamily,
commentstyle=\ttfamily\color{gray},
numbers=none,
numberstyle=\ttfamily\color{gray}\footnotesize,
stepnumber=1,
numbersep=5pt,
backgroundcolor=\color{white},
showspaces=false,
showstringspaces=false,
showtabs=false,
frame=none,
tabsize=4,
captionpos=b,
breaklines=true,
breakatwhitespace=false,
title=\lstname,
escapeinside={},
keywordstyle={},
morekeywords={}
}
\title[Problemas de Procesos I]{Problemas de Procesos Estoc\'asticos I\\ Posgrado en Ciencias Matem\'aticas\\ Universidad Nacional Aut\'onoma de M\'exico\\Tarea 6}
\author{Antonio Soriano Flores}
%\address{}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref} 
\input{definitions.tex}
%\usepackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\begin{document}
\maketitle
\begin{problema}
Un proceso estoc\'astico $B=\paren{B_t,t\geq 0}$ es un movimiento browniano en ley si y s\'olo si es un proceso gaussiano centrado y $\esp{B_sB_t}=s\wedge t$.\\

\end{problema}
\begin{proof}
$\Rightarrow:)$ Supongamos que $B=\paren{B_t,t\geq 0}$ es un movimiento browniano en ley por demostrar que $\paren{B_t,t\geq 0}$  es un proceso gaussiano centrado y que ademas $\esp{B_sB_ t}=s\wedge t$.\\
Primero, para demostrar que $\paren{B_t,t\geq 0}$  es un proceso gaussiano tenemos que ver que el vector ,$\paren{B_{t_1},B_{t_2},\ldots,B_{t_n}}$ donde  $(t_1<t_2,\ldots,<t_n)$ es un vector gaussiano lo que equivale  a probar que cualquier  combinaci\'on lineal sigue una distribuci\'on normal. Pero dado que cualquier combinaci\'on lineal la podemos escribir como:
$$
\sum_{i=1}^{n}\lambda_iB_{t_i} =\sum_{i=1}^{n}\alpha_i\paren{B_{t_i}-B_{t_{i-1}}}\sim Normal
$$
Notemos que esto \'ultimo es cierto debido a que por hip\'otesis $\paren{B_t,t\geq 0}$ es un movimiento browniano en ley  y por tanto $\paren{B_{t_i}-B_{t_{i-1}}} $ son Normales  independientes (definiendo $B_{t_0}=0$) y como combinaci\'on lineal finita de Normales independientes es Normal se sigue que  $\paren{B_{t_1},B_{t_2},\ldots,B_{t_n}}$  es un vector gaussiano que adem\'as es centrado porque cada $B_{t_i}$ tiene esperanza igual a cero,  lo que demuestra que $\paren{B_t,t\geq 0}$  es un proceso gaussiano centrado\\
Finalmente para probar que $\esp{B_sB_t}=s\wedge t$ lo haremos por casos:
\begin{itemize}
\item Supongamos $s=t$, entonces:
 $$
\esp{B_sB_t}=\esp{B_s^2}=\var{B_s^2}=s=t=s\wedge t
 $$
\item Supongamos $s < t$, entonces
$$
\esp{B_sB_t}=\esp{B_sB_t-B_s^2+B_s^2}=\esp{B_s(B_t-B_s)+B_s^2}=\esp{B_s(B_t-B_s)}+s
$$
$$
=\esp{(B_s-B_0)(B_t-B_s)}+s=\esp{B_s-B_0}\esp{B_t-B_s}+s=s=s\wedge t
$$  
\item De forma muy similar a lo anterior,  ahora  supongamos $t < s$, entonces:
$$
\esp{B_sB_t}=\esp{B_sB_t-B_t^2+B_t^2}=\esp{B_t(B_s-B_t)+B_t^2}=\esp{B_t(B_s-B_t)}+t
$$
$$
=\esp{(B_t-B_0)(B_s-B_t)}+t=\esp{B_t-B_0}\esp{B_s-B_t}+t=t=s\wedge t
$$  
\end{itemize}
En todos los casos se tiene que $\esp{B_sB_t}=s\wedge t$  lo que termina la primer parte de la prueba.\\
$\Leftarrow:)$  Ahora supongamos que $\paren{B_t,t\geq 0}$  es un proceso gaussiano centrado tal que $\esp{B_sB_t}=s\wedge t$, por demostrar que $B=\paren{B_t,t\geq 0}$ es un movimiento browniano en ley. Tenemos entonces que probar las siguientes propiedades:
\begin{itemize}
\item $B_0=0$. Esto es consecuencia del hecho de que $\esp{B_0^2}=\esp{B_0B_0}=0\wedge 0=0$, esto implica directamente que $B_0$  es una variable degenerada que toma el valor cero pues se tiene que $\esp{B_0}=0$ y $\var{B_0}=0$.
\item B tiene incrementos independientes. Sea $0\leq t_1<t_2,\ldots,<t_n$ queremos demostrar que $(B_{t_i}-B_{t_{i-1}})$ son variables aleatorias independientes. Primero, como B es un proceso gaussiano tenemos que el vector $\paren{B_{t_1},B_{t_2},\ldots,B_{t_n}}$ es un vector gaussiano centrado y por lo tanto  $B_{t_i}$ son v.a.  gaussinas de donde se sigue que el vector $\paren{B_{t_1}-B_0,B_{t_2}-B_{t_1},\ldots,B_{t_n}-B_{t_{n-1}}}$ es tambi\'en un vector gaussiano ya que nuevamente al hacer el producto punto y expresarlo como combinaci\'on lineal de las $B_{t_i}$'s:
$$
\sum_{i=1}^{n}\lambda_i\paren{B_{t_i}-B_{t_{i-1}}} =\sum_{i=1}^{n}\alpha_iB_{t_i}\sim Normal
$$ 
Obtenemos entonces una combinaci\'on lineal de las  $B_{t_i}$'s que por hip\'otesis  es Normal pues $(B_t)$ es un proceso Gaussiano, por lo anterior para verificar la independencia solo tenemos que probar  que la correlaci\'on entre las entradas de este vector son cero. En efecto, tomemos  la entrada $i$ y la entrada $j$ $ (i\neq j)$ de este vector y verifiquemos su correlaci\'on (Recordemos que tenemos un vector gaussiano centrado y por tanto el calculo de la correlaci\'on se reduce  a calcular la esperanza del producto de las variables aleatorias), sin p\'erdida de generalidad supondremos que $i < j$:
$$
\esp{\paren{B_{t_i}-B_{t_{i-1}})(B_{t_j}-B_{t_{j-1}}}}=\esp{B_{t_i}B_{t_j}-B_{t_i}B_{t_{j-1}}-B_{t_{i-1}}B_{t_j}+B_{t_{i-1}}B_{t_{j-1}}}
$$
$$
=\esp{B_{t_i}B_{t_j}}-\esp{B_{t_i}B_{t_{j-1}}}-\esp{B_{t_{i-1}}B_{t_j}}+\esp{B_{t_{i-1}}B_{t_{j-1}}}=t_i-t_i-(t_{i-1})+(t_{i-1})=0
$$
Por lo tanto se concluye que $(B_{t_i}-B_{t_{i-1}})$  son variables aleatorias normales e independientes por tener correlaci\'on cero.
\item $B_t \sim Normal(0,t)$. En efecto, pues  tenemos por hip\'otesis que $B_t$ es  v.a. normal centrada por lo tanto $\esp{B_t}=0$ luego como por hip\'otesis  $\esp{B_sB_t}=s\wedge t$ entonces $\var{B_t}=\esp{B_t^2}=\esp{B_tB_t}=t$ por lo tanto $B_t \sim Normal(0,t)$.
\item $B$ tiene incrementos estacionarios. Tenemos que probar que $B_{t+s}-B_t\stackrel{d}{=} B_s$.   Ya sabemos por el inciso anterior que  $B_t \sim N(0,t)$ , por otro lado notemos que  $B_{t+s}-B_t$ al ser combinaci\'on de  de un proceso Gaussiano  se concluye  que $B_{t+s}-B_t$ es normal, solo calculemos sus par\'ametros para verificar la igualdad en distribuci\'on:
$$
\esp{B_{t+s}-B_t}=\esp{B_{t+s}}-\esp{B_t}=0
$$
$$
\var{B_{t+s}-B_t}=\esp{(B_{t+s}-B_t)^2}=\esp{B_{t+s}^2}-2\esp{B_{t+s}B_t}+\esp{B_t^2}
$$
$$
=(t+s)-2t\wedge (t+s)+t=t+s-2t+t=s
$$
Por lo tanto concluimos que $B_{t+s}-B_t \sim N(0,s)$. Por otro lado por el inciso anterior sabemos que $B_s\sim N(0,s)$ por lo tanto tenemos que:
$$
B_{t+s}-B_t\stackrel{d}{=}B_s
$$
De donde concluimos que el proceso tiene incrementos estacionarios.
\end{itemize}
Finalmente  por los puntos anteriores   se concluye que $B=\paren{B_t,t\geq 0}$ es un movimiento browniano en ley.
\end{proof}
\begin{problema}
El objetivo de este problema es construir, a partir de movimientos brownianos en $[0,1]$, al movimiento browniano en $[0,\infty)$.
\begin{enumerate}
\item Pruebe que existe un espacio de probabilidad $\ofp$ en el que existe una sucesi\'on $B^1,B^2,\ldots$ de movimientos brownianos en $[0,1]$ independientes. (Sugerencia: utilice la construcci\'on del movimiento browniano de L\'evy  para que la soluci\'on sea corta.)
\begin{proof}
En la construcci\'on de del movimiento browniano de L\'evy utilizamos el espacio de probabilidad   $\ofp$  donde estuvieran definidas las variables aleatorias:
$$
\xi_{i,n}  \quad 0 \leq i \leq 2^n \quad n\geq 1  
$$
Tal que estas variables fueran distribuidas de forma Normal de par\'ametros $(0,1)$ y que fueran independientes. Dicho espacio sabemos que existe por lo visto en el capitulo 2 de las notas donde se construy\'o la sucesi\'on de variables aleatorias independientes a partir de una sucesi\'on de variables aleatorias Bernulli, luego se extendi\'o este resultado  a variables uniformes$(0,1)$  para que  finalmente y ,a partir de la funci\'on de cu\'antiles, se obtuviera una sucesi\'on de variables con distribuci\'on arbitrarias e independientes. \\
Para generalizar este resultado, ahora consideremos el espacio de probabilidad  $\ofp$   donde est\'en definidas las variables aleatorias:
$$
\xi_{i,n}^m  \quad 0 \leq i \leq 2^n \quad n\geq 1   \quad m\geq 1
$$
De tal forma que todas tengan distribuci\'on Normal Est\'andar con media 0 y varianza 1 y que sean independientes. Luego entonces para cada $m \in \mathds{N}$   podemos construir el proceso browniano $B^m$  en $[0,1]$, es decir con esto estaremos construyendo una infinidad numerable de movimientos Brownianos que ser\'an independientes por la forma en que se construyeron a partir de las variables $\xi_{i,n}^m$ que sabemos, por como se tomaron, que son independientes.
\end{proof}
\item Defina a $B_t=B^1_1+\cdots+B^{\floor{t}}_1+B^{\ceil{t}}_{t-\floor{t}}$ para $t\geq 0$. Pruebe que $B$ es un movimiento browniano. 
%\item Pruebe que $\paren{B_t}^2-t$ no tiene incrementos independientes. Sugerencia: En el ejercicio anterior identific\'o la distribuci\'on de $\paren{B_t}^2$; calcule la transformada de Laplace conjunta de dos incrementos.
\begin{proof}
Para probar que $B_t$ es un movimiento browniano necesitamos verificar las siguientes propiedades.
\begin{enumerate}
\item  $B_0=0$. En efecto pues por definici\'on y construcci\'on de $B^1$ obtenemos que:  $B_0=B^1_0=0$
\item $B_t \sim N(0,t)$. En efecto, como $B_t=B^1_1+\cdots+B^{\floor{t}}_1+B^{\ceil{t}}_{t-\floor{t}}$  entonces notemos que al ser $B^m$ movimientos brownianos independientes, entonces se sigue que $B_t$ es una combinaci\'on lineal de normales independientes y por tanto $B_t$ es normal. Veamos los par\'ametros:
$$
\esp{B_t}=\esp{B^1_1+\cdots+B^{\floor{t}}_1+B^{\ceil{t}}_{t-\floor{t}}}=0
$$
Lo anterior es valido porque cada $B^m$ es movimiento browniano y por tanto tienen media 0. Por otro lado para obtener la varianza  del proceso al tiempo $t$ se tiene
 que por la independencia de $B^m$:
 $$
\var{B_t}=\var{B^1_1+\cdots+B^{\floor{t}}_1+B^{\ceil{t}}_{t-\floor{t}}}=\sum_{i=1}^{\floor{t}}\var{B^i_1}+\var{B^{\ceil{t}}_{t-\floor{t}}}
$$
Luego como cada $B^i_1\sim N(0,1)$ y como $B^{\ceil{t}}_{t-\floor{t}} \sim N(0,{t-\floor{t}})$. Se sigue entonces que:
$$
\var{B_t}=\floor{t}+t-\floor{t}=t
$$
De donde concluimos que en efecto $B_t$ tiene distribuci\'on Normal  de par\'ametros $(0,t)$.
\end{enumerate}
Con estos dos puntos hemos demostrado que el proceso $B_t$ es  centrado, para verificar que $B_t$ es proceso Gaussiano tenemos que verificar que para cualquier combinaci\'on lineal de $B_t$ tiene una distribuci\'on normal:
$$
\sum_{i=1}^{n}\lambda_i B_{t_i} \sim Normal
$$
Sin embargo notemos que esto \'ultimo es cierto por el hecho de que cada $B_{t_i}$  es combinaci—n lineal de movimientos brownianos en $[0,1]$ que por construcci\'on son independientes y con distrubuci\'on Gaussiana. Aqu\'i solo hay que tener cuidado cuando indices $(t_{i_k})_{k=1}^{n_1}$ est\'en contenidos    en un mismo intervalo de la forma $[m,m+1]$ para alg\'un  $m \in \na$, en cuyo caso s\'olo tenemos que recordar  que 
$
(B^m_{t-\floor{t_1}},B^m_{t-\floor{t_2}},\ldots,B^m_{t-\floor{t_{n_1}}})
$
Es un vector gaussiano pues $B^m$ es un movimiento browniano en $[0,1]$ que adem\'as es independiente de los movimientos browmianos  $(B^n)_{n\neq m}$ \\
Con todo lo anterior hemos probado que $(B_t)$ es un proceso Gaussiano centrado, ahora probaremos que adem\'as se cumple la propiedad de que $\esp{B_tB_s}=s\wedge t$. La prueba de esto \'ultimo  se obtendr\'a por casos, si $s=t$ entonces, $\esp{B_tB_s}=\esp{B_t^2}=t=t\wedge s$ por lo tanto se cumple la propiedad, ahora sin perdida de generalidad supongamos que $t<s$.
\begin{itemize}
\item Caso 1: Supongamos que $\floor{t} \leq t<s\leq \ceil{t}$. En este caso tenemos que:
$$
B_t=B^1_1+\cdots+B^{\floor{t}}_1+B^{\ceil{t}}_{t-\floor{t}}
$$
$$
B_s=B^1_1+\cdots+B^{\floor{t}}_1+B^{\ceil{t}}_{s-\floor{t}}
$$
Entonces al multiplicar  $B_tB_s$ y recordando que los procesos $B^m$ son independientes tenemos que:
$$
\esp{B_tB_s}=\esp{(B^1_1)^2}+\cdots+\esp{(B^{\floor{t}}_1)^2}+\esp{B^{\ceil{t}}_{t-\floor{t}}B^{\ceil{t}}_{s-\floor{t}} }
$$
Luego como cada $B^i_1 \sim N(0,1)$  y como $B^{\ceil{t}}$ es un movimiento browniano en $[0,1]$ se tiene que $\esp{B^{\ceil{t}}_{t-\floor{t}}B^{\ceil{t}}_{s-\floor{t}} }=t-\floor{t} \wedge s-\floor{t}=t-\floor{t}$. Por lo tanto:
$$
\esp{B_tB_s}=\sum_{i=1}^{\floor{t}}\esp{(B^i_1)^2}+t-\floor{t}=\floor{t}+t-\floor{t}=t=t\wedge s
$$
\item Caso 2: Supongamos que $\floor{t} \leq t \leq \ceil{t} < s$. En este caso tenemos que:
$$
B_t=B^1_1+\cdots+B^{\floor{t}}_1+B^{\ceil{t}}_{t-\floor{t}}
$$
$$
B_s=B^1_1+\cdots+B^{\floor{t}}_1+B^{\ceil{t}}_1+B^{\ceil{t}+1}_1+\cdots+B^{\floor{s}}_1+B^{\ceil{s}}_{s-\floor{s}}
$$
Entonces al multiplicar  $B_tB_s$ y recordando que los procesos $B^m$ son independientes tenemos que:
$$
\esp{B_tB_s}=\esp{(B^1_1)^2}+\cdots+\esp{(B^{\floor{t}}_1)^2}+\esp{B^{\ceil{t}}_{t-\floor{t}}B^{\ceil{t}}_{1} }
$$
Luego como cada $B^i_1 \sim N(0,1)$  y como $B^{\ceil{t}}$ es un movimiento browniano en $[0,1]$ se tiene que $\esp{B^{\ceil{t}}_{t-\floor{t}}B^{\ceil{t}}_{1} }=t-\floor{t} \wedge 1=t-\floor{t}$. Por lo tanto:
$$
\esp{B_tB_s}=\sum_{i=1}^{\floor{t}}\esp{(B^i_1)^2}+t-\floor{t}=\floor{t}+t-\floor{t}=t=t\wedge s
$$
\end{itemize}
Los puntos anteriores prueban que entonces $\esp{B_tB_s}=t\wedge s$. Luego recapitulando tenemos que el proceso definido $(B_t, t\geq 0)$ es un proceso gaussiano  centrado que adem\'as cumple con la propiedad de que $\esp{B_tB_s}=t\wedge s$, por lo que usando el problema 1 de la tarea 6 concluimos que  $(B_t, t\geq 0)$ es un movimiento browniano en ley, por lo que solo faltar\'ia probar que tiene trayectorias continuas, sin embargo la continuidad de las trayectorias del  proceso $(B_t, t\geq 0)$ se obtiene por construcci\'on el proceso, ya que recordemos que cada $B^m$ es continuo en [0,1] y que ademas $B_0^m=0$ para a toda $m\in \mathds{N}$

\end{proof}
\end{enumerate}
\end{problema}
\begin{problema}
Pruebe que si $\tilde X$ es una modificaci\'on de $X$ entonces ambos procesos tienen las mismas distribuciones finito-dimensionales. Concluya que si
 $B$ es un movimiento browniano en ley y $\tilde B$ es una modificaci\'on de $B$ con trayectorias continuas entonces $\tilde B$ es un movimiento browniano. 
\end{problema}
\begin{proof}
Como $\tilde X$ es una modificaci\'on de $X$ entonces sabemos que $\p(X_t=\tilde X_t)=1$ para toda $t\geq 0$. Queremos probar que ambos procesos tienen las mismas distribuciones finito-dimensionales, es decir tenemos que probar que para $0\leq t_1, <t_2, \ldots,<t_n$  se tiene que:
$$
\paren{X_{t_1},X_{t_2},\ldots,X_{t_n}}\stackrel{d}{=}\paren{\tilde X_{t_1},\tilde X_{t_2},\ldots,\tilde X_{t_n}}
$$ 
Verificamos entonces la igualdad de distribuciones,  para ello definamos al evento $A_n$ como:
$$
A_n:=\set{X_{t_1}=\tilde X_{t_1},X_{t_2}=\tilde X_{t_2},\ldots, X_{t_n}=\tilde X_{t_n}}
$$
Por la condici\'on de que $\tilde X$ es una modificaci\'on de $X$  se tiene que $P(A_n)=1$ para toda $n$. En efecto, para verificar esto utilizar\'emos inducci\'on sobre n:
\begin{itemize}
\item Para $n=1$ se tiene la igualdad por definici\'on  pues $P(X_{t_1}=\tilde X_{t_1})=1$

\item Supongamos valido que $\p(A_n)=1$ por demostrar que $\p(A_{n+1})=1$. Como:
$$
\p(A_{n+1})=\p\paren{\set{X_{t_1}=\tilde X_{t_1},X_{t_2}=\tilde X_{t_2},\ldots, X_{t_n}=\tilde X_{t_n}},\set{X_{t_{n+1}}=\tilde X_{t_{n+1}}}}
$$
$$
=\p\paren{\set{A_n} \cap \set{X_{t_{n+1}}=\tilde X_{t_{n+1}}}}
$$
Pero notemos que por hip\'otesis de inducci\'on $\p(A_n)=1$ y como $\tilde X$ es una modificaci\'on de $X$  se tiene ademas que:  $\p(X_{t_{n+1}}=\tilde X_{t_{n+1}})=1$, se sigue entonces que la intesecci\'on de estos eventos tiene probabilidad 1, pues:
$$
1=\p(A_n)\leq \p\paren{\set{A_n} \cup \set{X_{t_{n+1}}=\tilde X_{t_{n+1}}}} \leq 1
$$
Entonces  $\p\paren{\set{A_n} \cup \set{X_{t_{n+1}}=\tilde X_{t_{n+1}}}}=1$ pero como:
\fontsize{8pt}{8pt} \selectfont 
$$
\p\paren{\set{A_n} \cup \set{X_{t_{n+1}}=\tilde X_{t_{n+1}}}} = \p(A_n)+\p\paren{\set{X_{t_{n+1}}=\tilde X_{t_{n+1}}}}-\p\paren{\set{A_n} \cap \set{X_{t_{n+1}}=\tilde X_{t_{n+1}}}}
$$
\fontsize{10pt}{10pt} \selectfont 
Se sigue entonces que:
$$
1=1+1-\p\paren{\set{A_n} \cap \set{X_{t_{n+1}}=\tilde X_{t_{n+1}}}}
$$
De donde se concluye que en efecto $\p(A_{n+1})=1$, lo que termina la prueba por inducci\'on.
\end{itemize}
Por otro lado, continuando con la prueba para mostrar la igualdad en distribuci\'on:
\fontsize{8pt}{8pt} \selectfont
\begin{equation}
\p(X_{t_1}\leq   x_1,X_{t_2} \leq x_2,\ldots,X_{t_n} \leq x_n)=\p\paren{X_{t_1} \leq   x_1,X_{t_2} \leq x_2,\ldots,X_{t_n} \leq x_n, \set{A_n \cup A_n^c}}
\end{equation}
$$
=\p\paren{X_{t_1} \leq   x_1,X_{t_2} \leq x_2,\ldots,X_{t_n} \leq x_n, A_n}+\p\paren{X_{t_1} \leq   x_1,X_{t_2} \leq x_2,\ldots,X_{t_n} \leq x_n, A_n^c}
$$
\fontsize{10pt}{10pt} \selectfont
Pero notemos que: $\p\paren{X_{t_1} \leq   x_1,X_{t_2} \leq x_2,\ldots,X_{t_n} \leq x_n, A^c}=0$ ya que como vimos $\p(A_n)=1$ entonces $\p(A_n^c)=0$, por lo tanto de la ecuaci\'on (1) tenemos que:
$$
\p(X_{t_1}\leq   x_1,X_{t_2} \leq x_2,\ldots,X_{t_n} \leq x_n)=\p\paren{X_{t_1} \leq   x_1,X_{t_2} \leq x_2,\ldots,X_{t_n} \leq x_n, A_n}
$$
$$
=\probac{X_{t_1} \leq   x_1,X_{t_2} \leq x_2,\ldots,X_{t_n} \leq x_n}{ A_n}\p(A_n)
$$
Luego, como $\p(A_n)=1$ tenemos entonces que:
$$
\p(X_{t_1}\leq   x_1,X_{t_2} \leq x_2,\ldots,X_{t_n} \leq x_n)=\probac{X_{t_1} \leq   x_1,X_{t_2} \leq x_2,\ldots,X_{t_n} \leq x_n}{ A_n}
$$
$$
=\probac{X_{t_1} \leq   x_1,X_{t_2} \leq x_2,\ldots,X_{t_n} \leq x_n}{ X_{t_1}=\tilde X_{t_1},X_{t_2}=\tilde X_{t_2},\ldots, X_{t_n}=\tilde X_{t_n} }
$$
$$
=\p(\tilde X_{t_1}\leq   x_1,\tilde X_{t_2} \leq x_2,\ldots, \tilde X_{t_n} \leq x_n)
$$
Lo que muestra que en efecto:
$$
\paren{X_{t_1},X_{t_2},\ldots,X_{t_n}}\stackrel{d}{=}\paren{\tilde X_{t_1},\tilde X_{t_2},\ldots,\tilde X_{t_n}}
$$ 
Por lo tanto ambos procesos tienen las mismas distribuciones finito-dimensionales. Ahora con este resultado podemos afirmar que si
 $B$ es un movimiento browniano en ley y $\tilde B$ es una modificaci\'on de $B$ con trayectorias continuas entonces $\tilde B$ es un movimiento browniano. En efecto, al ser $\tilde B$ es una modificaci\'on de $B$ tenemos  que ambos procesos tienen las mismas distribuciones finito-dimensionales y por tanto  $\tilde B$  ser\'a un proceso gaussiano centrado ademas se cumple que $\esp{\tilde B_t \tilde B_s}=\esp{B_tB_s}=t \wedge s$ y por lo tanto usando el problema 1 de esta tarea se afirmar’a que $\tilde B$ es un movimiento browniano en ley,  pero como adem\'as $\tilde B$   tiene trayectorias continuas entonces se afirma que $\tilde B$ es un movimiento browniano.
\end{proof}
\begin{problema}
Sea\begin{esn}
M^\lambda_t=e^{\lambda B_t-\lambda^2t/2}.
\end{esn}
\begin{enumerate}
\item Explique y pruebe formalmente por qu\'e, para toda $n\geq 1$, $\partial^n M^\lambda_t/\partial \lambda^n$ es una martingala. 
\begin{proof}
Primero notemos que $M^\lambda_t$ es martingala. En efecto, primero porque $M^\lambda_t$ es adaptado debido a que suponemos se est\'a utilizando la filtraci\'on can\'onica adem\'as dado que:
$$
M^\lambda_t=e^{\lambda B_t-\lambda^2t/2} \leq e^{\lambda B_t}
$$
Entonces:
$$
\esp{\abs{M^\lambda_t}}=\esp{M^\lambda_t}\leq \esp{e^{\lambda B_t}}=e^{\frac{t\lambda^2}{2}}
$$
La \'ultima igualdad se debe a que est\'amos calculando la generadora de momentos para $B_t$ que sabemos sigue una distribuci\'on Normal$(0,t)$. Lo anterior concluye entonces que $M^\lambda_t $ es integrable finalmente para probar la propiedad de martingala para $M^\lambda_t$ sea $s<t$, entonces:
$$
\espc{M^\lambda_t}{\F_s}=\espc{e^{\lambda B_t-\lambda^2t/2}}{\F_s}=e^{-\lambda^2t/2}\espc{e^{\lambda B_t}}{\F_s}
$$
$$
=e^{-\lambda^2t/2}\espc{e^{\lambda( B_t- B_s+B_s) }}{\F_s}=e^{-\lambda^2t/2+\lambda B_s}\esp{e^{\lambda( B_{t-s}) }}$$
$$
=e^{-\lambda^2t/2+\lambda B_s}e^{(t-s)\lambda^2/2}=e^{\lambda B_s-s\lambda^2/2}=M^\lambda_s
$$
Lo que implica entonces que $M^\lambda_t$ as\'i definida es una $\F_t$-martingala. Ahora veamos que las derivadas respecto a $\lambda$ son tambi\'en martingalas. Para la demostraci\'on  ocup\'aremos el siguiente teorema que nos permite intercambiar l\'imite con esperanza condicional:
\begin{theorem}
Sea$\ofp$ un espacio de probabilidad y   $f:\Omega \times [a,b] \rightarrow \mathds{R}$ una funci\'on tal que para toda $\lambda  \in [a,b]$ la funci\'on  $\omega \rightarrow f(\omega,\lambda)$ es medible e integrable y que adem\'as para cada $\omega \in \Omega$ la funci\'on $\lambda \rightarrow f(\omega,\lambda)$ es derivable. Suponga que existe una funci\'on  $g:\Omega \rightarrow \mathds{R}$ integrable tal que domina a la derivada para todo $\omega \in \Omega$ es decir:
$$
\abs{\frac{\partial}{\partial \lambda}f(\omega,\lambda)}\leq g(\omega) \quad \forall \omega
$$
Entonces:
$$
\espc{\frac{\partial}{\partial \lambda}f(\omega,\lambda)}{\G}=\frac{\partial}{\partial \lambda}\espc{f(\omega,\lambda)}{\G}
$$
\end{theorem}
\begin{proof}
La demostraci\'on se basa en utlizar el T.C.D.  Primero fijamos $\lambda\in [a,b]$ y definimos en $\ofp$ la funci\'on:
$$
X_n=n\paren{f\paren{\omega,\lambda+\frac{1}{n}}- f\paren{\omega,\lambda}}
$$ 
Como por hip\'otesis  la funci\'on $\lambda \rightarrow f(\omega,\lambda)$ es derivable se sigue que $X_n \rightarrow X$ donde $X= \frac{\partial}{\partial \lambda}f(\omega,\lambda)$. Luego tenemos  como:
$$
\abs{X_n}=n\abs{f\paren{\omega,\lambda+\frac{1}{n}}- f\paren{\omega,\lambda}}=n\abs{\int_{\lambda}^{\lambda+\frac{1}{n}}\frac{\partial}{\partial u}f(\omega,u) du} 
$$
$$
\leq n\int_{\lambda}^{\lambda+\frac{1}{n}}\abs{\frac{\partial}{\partial u}f(\omega,u)}du\leq  n\int_{\lambda}^{\lambda+\frac{1}{n}} g(\omega) du  = g(\omega)n\paren{\lambda +\frac{1}{n}-\lambda}=g(\omega)  
$$
Luego entonces tendr\'iamos que:
$$
\abs{X_n} \leq g \in L_1
$$
Por lo tanto la sucesi\'on es dominada por una funci\'on integrable por lo que usando el T.C.D. se tiene que:
$$
\espc{\frac{\partial}{\partial \lambda}f(\omega,\lambda)}{\G}=\espc{\lim_{n \rightarrow \infty}X_n}{\G}=\lim_{n \rightarrow \infty}\espc{X_n}{\G}
$$
Pero 
$$
\lim_{n \rightarrow \infty}\espc{X_n}{\G}=\lim_{n \rightarrow \infty}\espc{n\paren{f(\omega,\lambda+\frac{1}{n})-f(\omega,\lambda)}}{\G}
$$
$$
=\lim_{n \rightarrow \infty}n\paren{\espc{f(\omega,\lambda+\frac{1}{n})}{\G}- \espc{f(\omega,\lambda)}{\G}  }=\frac{\partial}{\partial \lambda}\espc{f(\omega,\lambda)}{\G}
$$
De donde concluimos que en efecto, se puede intercambiar la derivada con la esperanza condicional.
$$
\espc{\frac{\partial}{\partial \lambda}f(\omega,\lambda)}{\G}=\frac{\partial}{\partial \lambda}\espc{f(\omega,\lambda)}{\G}
$$
\end{proof}
Con este teorema procedemos a mostrar que  para toda $n\geq 1$, $\partial^n M^\lambda_t/\partial \lambda^n$ es una martingala. La demostraci\'on se har\'a por inducci\'on sobre $n$. 
\begin{itemize}
\item Caso $n=1$. Es claro que $\partial M^\lambda_t/\partial \lambda$ es $\F_t$-medible pues la derivada es un limite de funci\'ones que son $\F_t$-medibles. Por otro lado para mostrar que $\partial M^\lambda_t/\partial \lambda$  es integrable notamos que:
$$
\abs{\partial M^\lambda_t/\partial \lambda}=e^{\lambda B_t-\lambda^2t/2}\abs{(B_t-\lambda t)}\leq e^{\lambda B_t-\lambda^2t/2} \abs{B_t}+e^{\lambda B_t-\lambda^2t/2} \abs{\lambda t}
$$
$$
\leq e^{\lambda B_t} \abs{B_t}+e^{\lambda B_t} \abs{\lambda t}
$$
Como ya vimos $\esp{e^{\lambda B_t} \abs{\lambda t}}=\abs{\lambda t}\esp{e^{\lambda B_t}}=\abs{\lambda t}e^{t\lambda^2/2}\leq \infty$\\
Por otro lado verificaremos que $e^{\lambda B_t} \abs{B_t}$ es integrable, pues:
$$
\esp{e^{\lambda B_t}\abs{B_t}}=\int_{-\infty}^{\infty}\abs{u}e^{\lambda u} \frac{1}{\sqrt{2\pi t}}e^{-\frac{u^2}{2t}}du
$$
$$
=e^{t\lambda^2/2}\int_{-\infty}^{\infty}\abs{u}\frac{1}{\sqrt{2\pi t}}e^{-\frac{(u-t\lambda)^2}{2t}}du
$$
La \'ultima integral sabemos que es finita pues se trata de $\esp{\abs{U}}$ donde $U\sim Normal(\lambda t,t)$ . Luego de este resultado concluimos 2 cosas, primero que $\abs{\partial M^\lambda_t/\partial \lambda}$  esta dominada por $e^{\lambda B_t} \abs{B_t}+e^{\lambda B_t} \abs{\lambda t} \in L_1$ y segundo,   que  
la primer derivada de  $M^\lambda_t$ esta en $L_1$.\\
Finalmente para terminar con la demostraci\'on del caso $n=1$  necesitamos verificar que se cumple la propiedad martingala, sea entonces $s < t$, entonces, dado que ya vimos que $\partial M^\lambda_t/\partial \lambda$ es dominada entonces podemos llevar a cabo el intercambio de esperanza con derivada:
$$
\espc{\frac{\partial}{\partial \lambda} M^\lambda_t}{\F_s}=\frac{\partial}{\partial \lambda} \espc{ M^\lambda_t}{\F_s}=\frac{\partial}{\partial \lambda}M^\lambda_s
$$
Lo que concluye que en efecto $\frac{\partial}{\partial \lambda} M^\lambda_t$ es martingala
\item Supongamos que $\frac{\partial^n}{\partial \lambda^n} M^\lambda_t$ es martingala, por demostrar que  $\frac{\partial^{n+1}}{\partial \lambda^{n+1}} M^\lambda_t$. Dada la hipotesis de inducci\'on tenemos entonces que:
$$
\espc{\frac{\partial^{n+1}}{\partial \lambda^{n+1}} M^\lambda_t}{\F_s}=\espc{\frac{\partial}{\partial \lambda}  \frac{\partial^n}{\partial \lambda^n} M^\lambda_t}{\F_s}=^*\frac{\partial}{\partial \lambda} \espc{ \frac{\partial^n}{\partial \lambda^n} M^\lambda_t}{\F_s}
$$
$$
\frac{\partial}{\partial \lambda} \frac{\partial^n}{\partial \lambda^n} M^\lambda_s=\frac{\partial^{n+1}}{\partial \lambda^{n+1}}M^\lambda_s
$$
Lo que demostrar\'ia  que $\frac{\partial^{n+1}}{\partial \lambda^{n+1}} M^\lambda_t$ tiene la propiedad de martingala\\
(*) Faltaria probar que  $\frac{\partial^n}{\partial \lambda^n} M^\lambda_t $ es dominada por una funci\'on integrable para poder sacar el operador derivada de la esperanza condicional
\end{itemize}
\end{proof}
\item Sea $\imf{H_n}{x}=\paren{-1}^ne^{x^2/2}\frac{d^n}{dx^n}e^{-x^2/2}$. A $H_n$ se le conoce como en\'esimo polinomio de Hermite. Calc\'ulelo para $n\leq 5$. Pruebe que $H_n$ es un polinomio para toda $n\in\na$ y que $\partial^n M^\lambda_t/\partial \lambda^n=t^{n/2}\imf{H_n}{B_t/\sqrt{t}}M^\lambda_t$. 
\begin{proof}
Los primeros 6 polinomios de Hermite son:
$$
H_0(x)=1
$$
$$
H_1(x)=x
$$
$$
H_2(x)=x^2-1
$$
$$
H_3(x)=x^3-3x
$$
$$
H_4(x)=x^4-6x^2+3
$$
$$
H_5=x^5-10x^3+15x
$$
Ahora probaremos que $H_n$ es polinomio de grado $n$ para toda $n$. La prueba ser\'a por inducci\'on sobre $n$
\begin{itemize}
\item Caso $n=1$ ya sabemos que $H_1(x)=x$ y por tanto es polinomio de grado 1
\item Supongamos ahora que $H_n$ es polinomio de grado $n$  por demostrar que $H_{n+1}$ es un polinomio. Por hip\'otesis de inducci\'on tenemos que:
$$
\paren{-1}^ne^{x^2/2}\frac{d^n}{dx^n}e^{-x^2/2}=H_n(x)=(a_nx^n+a_{n-1}x^{n-1}+\cdots+a_0)
$$
Entonces:
$$
\frac{d^n}{dx^n}e^{-x^2/2}=\paren{-1}^ne^{-x^2/2}(a_nx^n+a_{n-1}x^{n-1}+\cdots+a_0)
$$
\end{itemize}
Derivando la expresi\'on anterior obtenemos:
$$
\frac{d^{n+1}}{dx^{n+1}}e^{-x^2/2}=\paren{-1}^ne^{-x^2/2}(a_nnx^{n-1}+a_{n-1}(n-1)x^{n-2}+\cdots+a_1)
$$
$$
+\paren{-1}^n(a_nx^n+a_{n-1}x^{n-1}+\cdots+a_0)e^{-x^2/2}(-1x)
$$
Por lo tanto, factorizando $\paren{-1}^{n+1}e^{-x^2/2}$:
$$
H_{n+1}(x)=\paren{-1}^ne^{x^2/2}\frac{d^{n+1}}{dx^{n+1}}e^{-x^2/2}
$$
$$
=\paren{a_nx^{n+1}+a_{n-1}x^{n}+\cdots+a_0x-a_nnx^{n-1}-a_{n-1}(n-1)x^{n-2}-\cdots-a_1}
$$
De donde concluimos que $H_{n+1}(x)$ es en efecto un polinomio de grado $n+1$\\
Finalmente para terminar este inciso tenemos que ver que $$\left.\frac{\partial^n}{\partial \lambda^n} M^\lambda_t\right|_{\lambda=0}=\left.t^{n/2}\imf{H_n}{B_t/\sqrt{t}}M^\lambda_t\right|_{\lambda=0}=t^{n/2}\imf{H_n}{B_t/\sqrt{t}}$$
Para su demostraci\'on consideremos a la funci\'on $f(\lambda)=M_t^{\lambda}$ y desarrollemos el polinomio de taylor alrededor de 0
\begin{equation}
f(\lambda)=M_t^{\lambda} =\sum_{i=0}^{\infty}\frac{\lambda^n}{n!}\left.\frac{\partial^n}{\partial \lambda^n} M^\lambda_t\right|_{\lambda=0}
\end{equation}
Por otro lado por la definici\'on de $M_t^\lambda$ tenemos:
$$
M_t^\lambda=e^{\lambda B_t-\lambda^2t/2}=e^{-\frac{1}{2}\paren{-2\lambda B_t + \lambda^2t}  }
$$
Completamos un trinomio cuadrado en el exponente.
$$
=e^{\frac{B_t^2}{2t}-\frac{1}{2}\paren{\frac{B_t^2}{t}-2\lambda B_t + \lambda^2t}  }=e^{\frac{B_t^2}{2t}-\frac{1}{2}\paren{\frac{B_t}{\sqrt{t}}- \lambda\sqrt{t}}^2  }
$$
entonces expresando esto \'ultimo como serie de taylor al rededor de $\lambda=0$ obtenemos:
\begin{equation}
f(\lambda)=\sum_{i=0}^{\infty}\frac{\lambda^n}{n!}\left.\frac{\partial^n}{\partial \lambda^n}e^{\frac{B_t^2}{2t}-\frac{1}{2}\paren{\frac{B_t}{\sqrt{t}}- \lambda\sqrt{t}}^2}\right|_{\lambda=0}=\sum_{i=0}^{\infty}\frac{\lambda^n}{n!}e^{\frac{B_t^2}{2t}}\left.\frac{\partial^n}{\partial \lambda^n}e^{-\frac{1}{2}\paren{\frac{B_t}{\sqrt{t}}- \lambda\sqrt{t}}^2}\right|_{\lambda=0}
\end{equation}
Nos concentraremos en encontrar una expresi\'on para 
$$
\left.\frac{\partial^n}{\partial \lambda^n}e^{-\frac{1}{2}\paren{\frac{B_t}{\sqrt{t}}- \lambda\sqrt{t}}^2}\right|_{\lambda=0}
$$
Para resolver este problema definamos las siguientes funciones:
$$
f(u):=e^{-\frac{u^2}{2}} \quad g(\lambda):=\paren{\frac{B_t}{\sqrt{t}}- \lambda\sqrt{t}}
$$
Entonces bajo esta definici\'on:
$$
\frac{\partial^n}{\partial \lambda^n}e^{-\frac{1}{2}\paren{\frac{B_t}{\sqrt{t}}- \lambda\sqrt{t}}^2}=\frac{\partial^n}{\partial \lambda^n}f(g(\lambda))
$$
Afirmaci\'on: bajo las condiciones de este problema se tiene que:
$$
\frac{\partial^n}{\partial \lambda^n}f(g(\lambda))=\paren{g^{(1)}(\lambda)}^nf^{(n)}(g(\lambda))
$$
Demostraci\'on por inducci\'on:
\begin{itemize}
\item Caso $n=1$. Usando regla de la cadena se tiene el resultado:
$$
\frac{\partial}{\partial \lambda}f(g(\lambda))=f^{(1)}(g(\lambda))g^{(1)}(\lambda)
$$
\item Supongamos que la formula es  v\'alida para $n$ por demostrar que es v\'alida para $n+1$
$$
\frac{\partial^{n+1}}{\partial \lambda^{n+1}}f(g(\lambda))=\frac{\partial}{\partial \lambda}\frac{\partial^{n}}{\partial \lambda^{n}}f(g(\lambda))=\frac{\partial}{\partial \lambda}\paren{g^{(1)}(\lambda)}^nf^{(n)}(g(\lambda))
$$
$$
=\paren{g^{(1)}(\lambda)}^n\frac{\partial}{\partial \lambda}f^{(n)}(g(\lambda))+f^{(n)}(g(\lambda))\frac{\partial}{\partial \lambda}\paren{g^{(1)}(\lambda)}^n
$$
Sin embargo por la definici\'on de la funci\'on $g$ mostraremos que $\frac{\partial}{\partial \lambda}\paren{g^{(1)}(\lambda)}^n=0$, en efecto pues:
$$
\frac{\partial}{\partial \lambda}\paren{g^{(1)}(\lambda)}^n=n\paren{g^{(1)}(\lambda)}^{n-1}g^{(2)}(\lambda)
$$
y como $g^{(2)}(\lambda)=\frac{\partial^2}{\partial \lambda^2}\paren{\frac{B_t}{\sqrt{t}}- \lambda\sqrt{t}}=0$ de donde se sigue que:
$$
\frac{\partial^{n+1}}{\partial \lambda^{n+1}}f(g(\lambda))=\paren{g^{(1)}(\lambda)}^n\frac{\partial}{\partial \lambda}f^{(n)}(g(\lambda))=\paren{g^{(1)}(\lambda)}^{n+1}f^{(n+1)}(g(\lambda))
$$
Por lo tanto prueba que la f\'ormula es v\'alida para toda $n$ 
\end{itemize}
Por lo anterior hemos probado que:
$$
\frac{\partial^n}{\partial \lambda^n}e^{-\frac{1}{2}\paren{\frac{B_t}{\sqrt{t}}- \lambda\sqrt{t}}^2}=\paren{g^{(1)}(\lambda)}^nf^{(n)}(g(\lambda))=\paren{-\sqrt{t}}^nf^{(n)}\paren{g(\lambda)}
$$
Pero notemos que por la definici\'on de $H_n$
$$
f^{(n)}(u)=\frac{\partial^n}{\partial \lambda^n}e^{-u^2/2}=(-1)^nH_n(u)e^{-u^2/2}
$$
Por lo tanto:
$$
\frac{\partial^n}{\partial \lambda^n}e^{-\frac{1}{2}\paren{\frac{B_t}{\sqrt{t}}- \lambda\sqrt{t}}^2}=\paren{-\sqrt{t}}^n(-1)^nH_n(g(\lambda))e^{-g(\lambda)^2/2}
$$
Sustituyendo  este resultado en la ecuaci\'on (3) obtenemos :
$$
f(\lambda)=\sum_{i=0}^{\infty}\frac{\lambda^n}{n!}e^{\frac{B_t^2}{2t}}\left.\paren{t}^{n/2}H_n(g(\lambda))e^{-g(\lambda)^2/2}  \right|_{\lambda=0}
$$
Pero como $g(0)=B_t/\sqrt{t}$ entonces:
\begin{equation}
f(\lambda)=\sum_{i=0}^{\infty}\frac{\lambda^n}{n!}\paren{t}^{n/2}H_n(B_t/\sqrt{t})
\end{equation}
Finalmente por la ecuaci\'on (2) y (4) y la unicidad del polinomio de taylor obtenemos que:
$$
\sum_{i=0}^{\infty}\frac{\lambda^n}{n!}\paren{t}^{n/2}H_n(B_t/\sqrt{t})=f(\lambda)=\sum_{i=0}^{\infty}\frac{\lambda^n}{n!}\left.\frac{\partial^n}{\partial \lambda^n} M^\lambda_t\right|_{\lambda=0}
$$
De donde obtemos el resultado que quer\'iamos probar:
$$
\left.\frac{\partial^n}{\partial \lambda^n} M^\lambda_t\right|_{\lambda=0}=\paren{t}^{n/2}H_n(B_t/\sqrt{t})
$$
\end{proof}
\item Pruebe que $t^{n/2}\imf{H_n}{B_t/\sqrt{t}}$ es una martingala para toda $n$ y calc\'ulela para $n\leq 5$. 
\begin{proof}
Por el inciso anterior sabemos que  $\left.\frac{\partial^n}{\partial \lambda^n} M^\lambda_t\right|_{\lambda=0}$ es martingala, se sigue entonces que $\paren{t}^{n/2}H_n(B_t/\sqrt{t})$ es martingala. Las primeras cinco martingalas son:\\
$
B_t
$\\
$
B_t^2-t
$\\
$
B_t^3-3B_tt
$\\
$
B_t^4-6B_t^2t+3t^2
$\\
$
B_t^5-10B_t^3t+15B_tt^2
$
 \end{proof}

\item Aplique muestreo opcional a las martingalas anteriores al tiempo aleatorio $T_{a,b}=\min\set{t\geq 0:B_t\in\set{-a,b}}$ (para $a,b>0$) con $n=1,2$ para calcular $\proba{B_{T_{a,b}}=b}$ y $\esp{T_{a,b}}$, ?Qu\'e concluye cuando $n=3,4$? ?` Cree que $T_{a,b}$ tenga momentos finitos de cualquier orden? Justifique su respuesta.
\begin{proof}
Primero, sabemos que $B_t$ es martingala y $T_{a,b} \wedge s$ es tiempo de paro acotado, luego entonces $B_{T_{a,b} \wedge s}$ es martingala acotada. Usando el muestro ¼opcional tenemos que:
$$
\esp{B_{T_{a,b} \wedge s}}=\esp{B_0}=0
$$
Sin embargo al ser $B_{T_{a,b} \wedge s}$ martingala acotada y como $B_{T_{a,b} \wedge s} \rightarrow B_{T_{a,b}}$  c.s, entonces usando teorema de la convergencia acotada se tiene que:
$$
\esp{B_{T_{a,b} \wedge s}} \rightarrow \esp{B_{T_{a,b}}}  \quad \Rightarrow  \esp{B_{T_{a,b}}} =\esp{B_0}=0
$$ 
Pero $B_{T_{a,b}}$ es una variable aleatoria que solo toma dos valores $\set{-a,b}$. Entonces:
$$
\esp{B_{T_{a,b}}}=-a\p(B_{T_{a,b}}=-a)+b\p(B_{T_{a,b}}=b)=0
$$
De donde usando el hecho de que $\p(B_{T_{a,b}}=-a)=1-\p(B_{T_{a,b}}=b)$ se concluye que:
$$
\p(B_{T_{a,b}}=-a)=\frac{b}{a+b} \quad \p(B_{T_{a,b}}=b)=\frac{a}{a+b}
$$
Ahora utilizando la segunda martingala  $B_t^2-t$ y bajo un argumento similar al anterior concluimos que:
$$
0=\esp{B_{T_{a,b}}^2-T_{a,b}}=a^2\p(B_{T_{a,b}}=-a)+b^2 \p(B_{T_{a,b}}=b)-\esp{T_{a,b}}
$$
Por lo tanto, sustituyendo el valor de la probabilidades que obtuvimos arriba concluimos que:
$$
\esp{T_{a,b}}=\frac{a^2b}{a+b}+\frac{b^2a}{a+b}=ab
$$
Ahora trabajaremos con $n=3$, la martingala  que obtenemos es $B_t^3-3B_tt$ de donde nuevamente aplicando el muestreo opcional obtenemos que:
$$
0=\esp{B_{T_{a,b}}^3-3B_{T_{a,b}}T_{a,b}}=\frac{b^3a-a^3b}{a+b}-3\esp{B_{T_{a,b}}T_{a,b}}
$$
De donde concluimos que:
$$
\esp{B_{T_{a,b}}T_{a,b}}=\frac{b^3a-a^3b}{3(a+b)}
$$
De esta \'ultima expresi\'on recordemos que $B_{T_{a,b}}=-a\mathds{1}_{\set{B_{T_{a,b}}=-a}}+b\mathds{1}_{\set{B_{T_{a,b}}=b}}$ entonces:
\begin{equation}
\frac{b^3a-a^3b}{3(a+b)}=\esp{B_{T_{a,b}}T_{a,b}}=-a\esp{T_{a,b}\mathds{1}_{\set{B_{T_{a,b}}=-a}}}+b\esp{T_{a,b}\mathds{1}_{\set{B_{T_{a,b}}=b}}}
\end{equation}
Por otro lado $T_{a,b}=T_{a,b}\mathds{1}_{\set{B_{T_{a,b}}=-a}}+T_{a,b}\mathds{1}_{\set{B_{T_{a,b}}=b}}$ entonces:
\begin{equation}
ab=\esp{T_{a,b}}=\esp{T_{a,b}\mathds{1}_{\set{B_{T_{a,b}}=-a}}}+\esp{T_{a,b}\mathds{1}_{\set{B_{T_{a,b}}=b}}}
\end{equation}
De las ecuaciones (4) y (5) obtenemos un sistema de ecuaciones de donde al resolverlo obtenemos:
$$
\esp{T_{a,b}\mathds{1}_{\set{B_{T_{a,b}}=b}}}=\frac{b^3a-a^3b}{3(a+b)}+\frac{a^2b}{a+b}
$$
$$
\esp{T_{a,b}\mathds{1}_{\set{B_{T_{a,b}}=-a}}}=ab-\frac{b^3a-a^3b}{3(a+b)}-\frac{a^2b}{a+b}
$$
Caso $n=4$ tenemos  la siguiente martingala $B_t^4-6B_t^2t+3t^2$ de donde nuevamente aplicando el muestro opcional obtenemos:
$$
0=\esp{B_{T_{a,b}}^4-6B_{T_{a,b}}^2{T_{a,b}}+3T_{a,b}^2}
$$
De donde despejando:
$$
\esp{T_{a,b}^2}=\frac{1}{3}\paren{\esp{6B_{T_{a,b}}^2{T_{a,b}}}-\esp{B_{T_{a,b}}^4}}=2\esp{B_{T_{a,b}}^2{T_{a,b}}}-\frac{a^4b+b^4a}{3(a+b)}
$$
De esta \'ultima expresi\'on trabajar\'emos  $\esp{B_{T_{a,b}}^2{T_{a,b}}}$. Como:
$$
B_{T_{a,b}}^2=a^2\mathds{1}_{\set{B_{T_{a,b}}=-a}}+b^2\mathds{1}_{\set{B_{T_{a,b}}=b}}
$$
Entonces:
$$
\esp{B_{T_{a,b}}^2{T_{a,b}}}=a^2\esp{T_{a,b}\mathds{1}_{\set{B_{T_{a,b}}=-a}}}+b^2\esp{T_{a,b}\mathds{1}_{\set{B_{T_{a,b}}=b}}}
$$
$$
a^2\paren{ab-\frac{b^3a-a^3b}{3(a+b)}-\frac{a^2b}{a+b}}+b^2\paren{\frac{b^3a-a^3b}{3(a+b)}+\frac{a^2b}{a+b}}
$$
Por lo tanto:
$$
\esp{T_{a,b}^2}=2\paren{a^2\paren{ab-\frac{b^3a-a^3b}{3(a+b)}-\frac{a^2b}{a+b}}+b^2\paren{\frac{b^3a-a^3b}{3(a+b)}+\frac{a^2b}{a+b}}
} -\frac{a^4b+b^4a}{3(a+b)}
$$
Lo que muestra que tiene segundo momento finito. Ahora bien, notemos que para calcular el n-\'esimo momento de $T_{a,b}$ requeriremos de la martingala que se obtiene de derivar $2n$ veces a la funci\'on $M_t^\lambda$ y evaluarla en $\lambda=0$, luego como ya probamos que evaluando $\lambda=0$ se tiene la igualdad $\partial^n M^\lambda_t/\partial \lambda^n=t^{n/2}\imf{H_n}{B_t/\sqrt{t}}$, entonces obtendr\'iamos que la martingala para calcular $\esp{T_{a,b}^n}$ es de la forma:
$$
t^{n}\imf{H_{2n}}{B_t/\sqrt{t}}=t^n\paren{a_{2n}\paren{\frac{B_t}{\sqrt t}}^{2n}+a_{2n-2}\paren{\frac{B_t}{\sqrt t}}^{2n-2}+\ldots+a_0}
$$
De donde al aplicar el muestro opcional  para martingalas obtendr’amos que $\esp{T_{a,b}^n}$  queda en funci\'on de:
 $$\esp{B_{T_{a,b}}^{2n}}, \esp{B_{T_{a,b}}^{2n-2}T_{a,b}},\esp{B_{T_{a,b}}^{2n-4}T_{a,b}^2},\ldots ,\esp{B_{T_{a,b}}^{2}T_{a,b}^{n-1}}$$
Luego cada una de estas esperanzas es finita pues depende b\'asicamente de esperanzas que fueron calculadas  de martingalas anteriores. Por lo que concluimos que $T_{a,b}$ tiene todos sus momentos finitos. 
\end{proof}
\item Aplique el teorema de muestreo opcional a la martingala $M^\lambda $ al tiempo aleatorio $T_a=\inf\set{t\geq 0:B_t\geq a}$ si $\lambda>0$. Diga por qu\'e es necesaria la \'ultima hip\'otesis y calcule la transformada de Laplace de $T_a$. 
\begin{proof}
Primero, ya probamos que $M_t^\lambda$ es martingala, luego $T_a\wedge s$ es tiempo de paro acotado y adem\'as $M_{T_a\wedge s}^\lambda$ es martingala, usando el muestreo opcional obtenemos:
$$
\esp{M_{T_a\wedge s}^\lambda}=\esp{M_0^\lambda}=1
$$
Pero como $M_{T_a\wedge s}^{\lambda}$ es positiva solo ten\'emos que encontrar una cota superior al proceso para asegurar que ten\'emos una martingala acotada, sin embargo notando que si $s\leq T_a$ entonces $M_{T_a\wedge s}^{\lambda}=M_s^{\lambda}=e^{\lambda B_s-\lambda^2s/2} \leq e^{\lambda a}$  y si $s > T_a$ entonces $M_{T_a\wedge s}^{\lambda}=M_{T_a}^{\lambda}=e^{\lambda a -\lambda^2s/2} \leq e^{\lambda a}$ (aqu\'i hacemos uso de la continuidad del proceso $B_t$ asi como del hecho de que $\lambda >0$). Luego entonces hemos probado que $M_{T_a\wedge s}^{\lambda}$  es una martingala acotada, por lo tanto dado que  $M_{T_a\wedge s}^{\lambda} \rightarrow M_{T_a}^{\lambda}$  y usando el teorema de convergencia acotada:
$$
1=\esp{M_{T_a\wedge s}^\lambda}\rightarrow \esp{M_{T_a}^{\lambda}}
$$
Por lo tanto:
$$
\esp{e^{\lambda a-\lambda^2T_a/2} }=1
$$
Entonces despejando:
$$
\esp{e^{-\lambda^2T_a/2}}=e^{-\lambda a}
$$
Para obtener la funci\'on generadora de momentos hacemos el cambio $u=\lambda^2/2$ de donde $\lambda=\sqrt{2u}$  entonces:
$$
\esp{e^{-uT_a}}=e^{- a\sqrt{2u}}
$$
\end{proof}
\item Opcional (para subir calificaci\'on en esta u otra tarea): 
\begin{enumerate}
\item Modifique el ejercicio para que aplique al proceso Poisson.
\item Resu\'elva el ejercicio modificado. 

\begin{proof}
Primero denotaremos $N_t$ como un proceso Poisson de Tasa $\gamma$ lo que implica que $N_t$ sigue una distribuc\'on Poisson($\gamma t$). Luego dado que la funci\'on generadora de momentos de una variable poisson de tasa  $\gamma t$ es:
$$
\esp{e^{\lambda N_t}}=e^{\gamma t\paren{e^{\lambda}-1}}
$$
Entonces definiremos a $M_t^{\lambda}$ como sigue:
$$
M_t^{\lambda}:=e^{\lambda N_t-\gamma t\paren{e^{\lambda}-1}}
$$
Probaremos entonces que as’ definida $M_t^{\lambda}$ es martingala respecto a la filtraci\'on can\'onica $\F_t=\sigma\paren{N_s: s\leq t}$. Sea entonces $s < t$;
$$
\espc{M_t^{\lambda}}{\F_s}=\espc{e^{\lambda N_t-\gamma t\paren{e^{\lambda}-1}}}{\F_t}=e^{-\gamma t\paren{e^{\lambda}-1}}\espc{e^{\lambda N_t}}{\F_s}
$$
Pero al ser $N_t$ un proceso poisson entonces tenemos incrementos independientes y estacionarios por lo tanto:
$$
\espc{M_t^{\lambda}}{\F_s}=e^{-\gamma t\paren{e^{\lambda}-1}}\espc{e^{\lambda (N_t-N_s+N_s)}}{\F_s}=e^{-\gamma t\paren{e^{\lambda}-1}+\lambda N_s}\esp{e^{\lambda (N_t-N_s)}}
$$
Entonces, como $N_t-N_s \stackrel{d}{=}  N_{t-s} \sim Poisson(\gamma (t-s))$ por lo tanto, dada la expresi\'on para la generadora de momentos de una Poisson se tiene:
$$
\esp{e^{\lambda (N_t-N_s)}}=e^{\gamma (t-s)\paren{e^{\lambda}-1}}
$$
Sustiyendo se tiene que:
$$
\espc{M_t^{\lambda}}{\F_s}=e^{-\gamma t\paren{e^{\lambda}-1}+\lambda N_s}e^{\gamma (t-s)\paren{e^{\lambda}-1}}=e^{\lambda N_s-\gamma s \paren{e^{\lambda}-1}}=M_s^{\lambda}
$$
Por lo tanto $M_t^{\lambda}$ es martingala. Luego para probar que la n-\'esima derivada respecto de $\lambda$ es martingala procederemos nuevamente por inducci\'on y por tanto necesitaremos dominar a  $\frac{\partial}{\partial \lambda}M_t^{\lambda}$ para poder llevar a cabo el intercambio de derivada con la esperanza condicional. Derivando tenemos:
$$
\abs{\frac{\partial}{\partial \lambda}M_t^{\lambda}}=\abs{M_t^{\lambda}\paren{N_t-\gamma t e^{\lambda}}}\leq M_t^{\lambda}\abs{N_t-\gamma t e^{\lambda}}\leq M_t^{\lambda}N_t + M_t^{\lambda}\gamma t e^{\lambda}
$$
Claramente al ser $M_t^{\lambda}$ martingala se sigue que $M_t^{\lambda}\gamma t e^{\lambda} \in L_1$ por lo que solo hay que verificar que  $M_t^{\lambda}N_t \in L_1$, veamos:
$$
\esp{M_t^{\lambda}N_t}=\esp{e^{\lambda N_t-\gamma t\paren{e^{\lambda}-1}}N_t}=e^{-\gamma t\paren{e^{\lambda}-1}}\esp{e^{\lambda N_t}N_t}
$$
Por tanto hay que verificar que $\esp{e^{\lambda N_t}N_t}$ es finita, recordando que $N_t \sim Poisson(\gamma t)$ tenemos:
$$
\esp{e^{\lambda N_t}N_t}=\sum_{x=0}^{\infty}{e^{\lambda x}x \frac{\paren{\gamma t}^x}{x!}e^{-\gamma t}}=e^{-\gamma t+\gamma t e^{\lambda}}\sum_{x=0}^{\infty}{x \frac{\paren{\gamma t e^{\lambda}}^x}{x!}}e^{\gamma t e^{\lambda}}
$$
Definiendo a $Y \sim Poisson\paren{\gamma t e^{\lambda}}$ entonces:
$$
\esp{e^{\lambda N_t}N_t}=e^{-\gamma t+\gamma t e^{\lambda}}\esp{Y}=e^{-\gamma t+\gamma t e^{\lambda}}\gamma t e^{\lambda} < \infty
$$
Por lo tanto hemos verificado que $e^{\lambda N_t}N_t \in L_1$, por lo tanto:
$$
\abs{\frac{\partial}{\partial \lambda}M_t^{\lambda}} \leq M_t^{\lambda}N_t + M_t^{\lambda}\gamma t e^{\lambda} \in L_1
$$
Lo anterior nos garantiza que podemos llevar a cabo el intercambio entre derivada y esperanza condicional por lo que nuevamente al usar el argumento inductivo se prueba que en efecto:
$\frac{\partial^n}{\partial \lambda^n}M_t^{\lambda}$ es una martingala (Faltar\'ia argumentar porque la n-\'esima derivada es dominada por una funci\'on integrable)\\
Ahora definamos la relaci\'on de las martingalas que se generan a partir de la derivaci\'on haciendo $\lambda=0$ con los polinomios de Hermite.
Demostraremos que en este caso:
$$
\left.\frac{\partial^n}{\partial \lambda^n}M_t^{\lambda}\right|_{\lambda=0}=\left.(\gamma t)^{n/2}H_n\paren{\frac{Nt-\gamma t}{\sqrt{\gamma t}}}M_t^{\lambda}\right|_{\lambda=0}
$$
(Falta la prueba y al parecer solo funciona con n=1 y 2)\\
Las martingalas que obtenemos de derivar directamente a $M_t^\lambda$ (no us\'e polin\'omio de Hermite sino que deriv\'e directamente $M_t^\lambda$ usando un software para derivar) :
$$
N_t-\gamma t
$$
$$
(N_t-\gamma t)^2-\gamma t
$$
$$
(N_t-\gamma t)^3-3(N_t-\gamma t)\gamma t-\gamma t 
$$
Luego al definir el tiempo aleatorio $T_{b}=\min\set{t\geq 0:N_t\in\set{b}}$ (para $b \in \na$) y al aplicar el muestreo opcional de Doob a la primer martingala obtenemos:
$$
\esp{N_{T_{b}}}- \gamma\esp{T_{b}}=0
$$
De donde concluimos que: $$\esp{T_b}=\frac{b}{\gamma}$$
Observaci\'on: Dado que $N_t$ es proceso Poisson entonces sabemos que los tiempos entre cambio de estados es Exponencial de par\'ametro ($\gamma$) luego entonces al ser el proceso creciente, se tiene que el tiempo de paro  $T_b$ coincide con tiempo que tarda el proceso Poisson en tener $b$ cambios de estados, es decir $T_b$ sigue la misma distribucii\'on que de la suma de $b$ exponenciales independientes,es decir, $T_b$ en este caso sigue una distribuc\'on Gamma$(b,\gamma)$ de donde concluimos que nuestro resultado obtenido v\'ia martingalas coincide con el hecho de que $\esp{T_b}=b/\gamma$.\\
Tomando ahora la martingala generada a partir de la segunda derivada y tras aplicar el muestro opcional de Doob tenemos los siguiente:
$$
\esp{\paren{N_{T_b}-\gamma T_b}^2-\gamma T_b}=0
$$
De donde despejando el segundo momento de $T_b$
$$
\esp{T_b^2}=\frac{((2b+1)\gamma)\esp{T_b}-b^2}{\gamma^2}=\frac{b(b+1)}{\gamma^2}
$$
Lo cual coincide con el segundo momento de una distribuci\'on Gamma$(b,\gamma)$. Finalmente utilizando la tercer martingala  y tras aplicar el muestro opcional de Doob tenemos los siguiente:
$$
\esp{(N_{T_b}-\gamma T_b)^3}-\esp{3(N_{T_b}-\gamma T_b)\gamma T_b}-\esp{\gamma T_b}=0 
$$
 De donde despejando el tercer momento obtenemos:
 $$
 \esp{T_b^3}=\frac{b^3+3\gamma^2(b+1)\esp{T_b^2}-\gamma(3b^2+3b+1)\esp{T_b}}{\gamma^3}
 $$
 Al sustituir el segundo y primero obtenemos
 $$
 \esp{T_b^3}=\frac{b(b+1)(b+2)}{\gamma^3}
 $$
 El cual coincide con el tercer momento de la distribuci\'on  Gamma$(b,\gamma)$. En general vemos que el momento $n$ depende de los primeros $n-1$ momentos y por tanto tiene todos sus momento finitos.\\
Para terminar con el ejercido ejercicio calcular\'emos la funci\'on generadora de momentos  del tiempo de paro  $T_b$. Para ello ocuparemos la martingala $M_t^\lambda$. Nuevamente haciendo uso del muestro opcional de doob  (Me parece que en este caso no necesitamos que $\lambda>0$ ya que el proceso $N_t$ es creciente)
$$
\esp{M_{T_b}^\lambda}=\esp{M_0^\lambda}=1
$$
Lo anterior es cierto porque la martingala $M_{T_a\wedge s}^\lambda$ es acotada por $e^{b\lambda}$
Obtenemos entonces:
$$
\esp{e^{b\lambda-\gamma T_b(e^\lambda-1}}=1
$$
De donde despejando:
$$
\esp{e^{-\gamma T_b(e^{\lambda}-1})}=e^{-b\lambda}=\paren{e^{\lambda}}^{-b}
$$
Haciendo el cambio de variable $u=-\gamma \paren{e^{\lambda-1}}$ de donde $e^\lambda=\paren{1-\frac{u}{\gamma}}$. Entonces obtenemos:
$$
\esp{e^{uT_b}}=\paren{1-\frac{u}{\gamma}}^{-b}
$$
Es decir $T_b$ fine la misma funci\'on generadora de momento que una variable aleatoria con distirbuci\'on Gamma$(b,\gamma)$ lo cual era de esperarse.
 \end{proof}
\end{enumerate}
\end{enumerate}
\end{problema}
\begin{problema}\mbox{}
\begin{enumerate}
\item Al aplicar la desigualdad maximal de Doob sobre los racionales de orden $n$ y pasar al l\'imite conforme $n\to\infty$, pruebe que $\sup_{t\in [0,1]}\abs{B_t}$ es cuadrado integrable.
\begin{proof}
Consideremos a la partici\'on di\'adica del [0,1], es decir:
$$
D_n=\set{\frac{k}{2n}: k=0,1,\ldots n}
$$
Consideremos ahora para cada $n$ a la martingala a tiempo discreto $\paren{B^n_k : k \in D_n}$. Luego al ser la funci\'on valor absoluto convexa se sigue tras aplicar la desigualdad de Jensen que  $\paren{\abs{B^n_k} : k \in D_n}$ es sub-martingala, luego entonces al aplicar la desigualdad maximal de Doob para p=2 obtenemos (Proposici\'on 1.5 de las notas) :
$$
\|  \max_{k\in D_n}\abs{B^n_k} \|_2\leq\frac{2}{2-1}\|\abs{B^n_1}\|_2
$$
Entonces:
$$
\esp{ (\max_{k\in D_n}\abs{B^n_k})^2} \leq 4\esp{\abs{B^n_1}^2}=4(\var{B_1})=4 
$$
Notemos que la desigualdad es v\'alida para toda $n$, luego dado de que la partici\'on $D_n$ se va refinando y que $B_t$ tiene trayectorias continuas, tenemos que al hacer $n \to \infty$:
$$
\max_{k\in D_n}\abs{B^n_k}\uparrow \sup_{t\in [0,1]}\abs{B_t} \quad (c.s)
$$
Luego entonces al usar Teorema de la convergencia mon\'otona:
$$
\esp{ \paren{\sup_{t\in [0,1]}\abs{B _t}}^2}=\lim_{n\to \infty}\esp{ (\max_{k\in D_n}\abs{B _k})^2} \leq 4
$$
Lo que muestra que es cuadrado integrable.
\end{proof}
\item Pruebe que la sucesi\'on de variables aleatorias\begin{esn}
\paren{\sup_{t\in [0,1]}\abs{B_{n+t}-B_n},n\in\na}
\end{esn}son independientes, id\'enticamente distribuidas y de media finita. (Utilice la propiedad de Markov.)
\begin{proof}
Como $B_t$ es movimiento browniano entonces:
$$
B_{n+t}-B_n \stackrel{d}{=} B_t \Rightarrow \abs{B_{n+t}-B_n} \stackrel{d}{=} \abs{B_t}
$$
Luego considerando a la funci\'on medible  $F: C[0,1] \to \re$ dada por:
$$
F(f)=\max_{t\in[0,1]} f(t)=\sup_{t\in[0,1]} f(t)
$$
Se sigue que:
$$
 \sup_{t\in [0,1]}\abs{B_{n+t}-B_n} \stackrel{d}{=}  \sup_{t\in [0,1]}\abs{B_{t}} \quad n \in \na
 $$
 Por lo tanto se tiene que  $\sup_{t\in [0,1]}\abs{B_{n+t}-B_n} $ tiene la misma distirbuci\'on que $\sup_{t\in [0,1]}\abs{B_{t}} $ para toda $n$ y por tanto son identicamentes distribuidas. Luego, pare verificar que tienen media finita recordemos que por el inciso anterior  $\esp{ \paren{\sup_{t\in [0,1]}\abs{B _t}}^2} < \infty$ por lo tanto  $$\sup_{t\in [0,1]}\abs{B _t} \in L_2$$ de donde se sigue que $\esp{\sup_{t\in [0,1]}\abs{B _t}} < \infty$ luego por lo anterior:
 $$
 \esp{ \sup_{t\in [0,1]}\abs{B_{n+t}-B_n} }=\esp{ \sup_{t\in [0,1]}\abs{B_{t}} } < \infty
  $$
  De donde se concluye que tiene media finita. Finalmente para demostrar que son independientes recordemos que al ser $B_t$ movimiento browniano entonces
  $$
 \paren{B_{n+t}-B_n, n \in \na }
  $$ 
  Son variables aleatorias independientes y adem\'as por la propiedad de markov:
  $$
  B_{n+t}-B_n \perp \sigma(B_s: s\leq n)
  $$
  Entonces los  proceso brownianos restringido a $[0,1)$ dado por: $$\paren{B^n_t=B_{n+t}-B_n, t\in [0,1]}$$  Tiene trayectorias independientes para cada n, de donde tomando supremos conciu\'imos que en efecto:
  $$
  \paren{\sup_{t\in [0,1]}\abs{B_{n+t}-B_n},n\in\na} 
  $$
  Son v.a. independientes.
  
\end{proof}
\item Al utilizar Borel-Cantelli, pruebe que, para cualquier $C>0$ fija\begin{esn}
\limsup_{n\to\infty}\sup_{t\in [0,1]}\abs{B_{n+t}-B_n}/n\leq C\end{esn} casi seguramente.
\begin{proof}
El lema de Borel Cantelli nos dice que si $\set{A_n}$ es una sucesi\'on de eventos independientes  entonces si:
$$
\sum_{n=1}^{\infty}\p(A_n)=\infty \quad \Rightarrow \p(\limsup A_n)=1 
$$ 
Definamos entonces a la sucesi\'on eventos como :
$$
A_n=\set{\sup_{t\in [0,1]}\abs{B_{n+t}-B_n}/n\leq C}
$$
Por el inciso anterior del ejercicio sabemos que los $A_n$'s  son independientes, entonces si probamos que $\sum_{n=1}^{\infty}\p(A_n)=\infty$ el lema de Borel Cantelli nos indicar\'ia que
$$
\p\paren{\limsup\sup_{t\in [0,1]}\abs{B_{n+t}-B_n}/n\leq C}=1 
$$
Lo que terminar\'ia la prueba, nos concentraremos entonces en probar que $$\sum_{n=1}^{\infty}\p(A_n)=\infty$$ Para demostrar esto recordemos  la desigualdad de Markov para  la cual nos dice que :
$$
\p(\abs{X}>x)\leq \frac{\esp{\abs{X}^r}}{x^r}
$$
Dado que $\sup_{t\in [0,1]}\abs{B_{n+t}-B_n}\stackrel{d}{=} \sup_{t\in [0,1]}\abs{B_{t}}$  entonces utilizando la desiguladad de Markov con $r=2$
\begin{align}
\p(A_n^c)&=\p\paren{\sup_{t\in [0,1]}\abs{B_{n+t}-B_n}> nC}=\p\paren{\sup_{t\in [0,1]}\abs{B_{t}}> nC}\notag \\ &\leq  \frac{\esp{\paren{\sup_{t\in [0,1]}\abs{B_{t}}}^2}}{(nC)^2} \notag
\end{align}
Luego entonces:
$$
\p(A_n)=1-\p(A_n^c) > 1-  \frac{\esp{\paren{\sup_{t\in [0,1]}\abs{B_{t}}}^2}}{(nC)^2}
$$
Si tomamos limite cuando $n \to \infty$ y recordando que  $\esp{\paren{\sup_{t\in [0,1]}\abs{B_{t}}}^2} < \infty$ concluimos que:
$$
 \lim_{n \to \infty}\p(A_n)=1  \quad \Rightarrow \sum_{n=1}^{\infty}\p(A_n)=\infty
 $$
 Lo que termina la prueba.
\end{proof}
\item Pruebe que $\paren{B_n/n,n\geq 1}$ converge casi seguramente a $0$ y deduzca que\begin{esn}
\lim_{t\to\infty }B_t/t=0.
\end{esn}
\begin{proof}
Primero notemos que:
$$
B_n=B_n-B_{n-1}+B_{n-1}-B_{n-2}+\ldots+B_2-B_1+B_1-B_0
$$
$$
B_n=\sum_{k=1}^{n}B_k-B_{k-1}
$$
Dado que $B_t$ tiene incrementos estacionarios  se sigue que $B_k-B_{k-1}\sim N(0,1)$ y por tonto usando la ley fuertes de los grandes n\'umeros obtenemos que:
$$
\lim_{n \to \infty }\frac{B_n}{n}=\lim_{n \to \infty }\frac{\sum_{k=1}^{n}B_k-B_{k-1}}{n}=0
$$
Ahora probaremos que $\lim_{t \to \infty}B_t/t=0$ para ello tomemos $t \in \re^+$ entonces  sabemos existe $n \in \na$ tal que $n\leq t < n+1$ y por tanto existe $s \in [0,1]$ tal que: $t=n+s$. Entonces:
$$
\frac{\abs{B_t}}{t}=\frac{\abs{B_n+s}}{n+s} \leq \frac{\abs{B_{n+s}}}{n}= \frac{\abs{B_{n+s}-B_n+B_n}}{n}\leq  \frac{\abs{B_{n+s}-B_n}}{n}+\frac{\abs{B_n}}{n}
$$
$$
\leq  \frac{\sup_{s \in[0,1]}{\abs{B_{n+s}-B_n}}}{n}+\frac{\abs{B_n}}{n}
$$
Luego si tomamos l\'imite  $t \to  \infty$ entonces $n \to  \infty$ pues recordemos que $n\leq t < n+1$, entonces:
\begin{align}
\limsup_{t\to\infty }\frac{\abs{B_t}}{t} &\leq \limsup_{n \to \infty}\paren{ \frac{\sup_{s \in[0,1]}{\abs{B_{n+s}-B_n}}}{n}+\frac{\abs{B_n}}{n}} \notag\\
&=\limsup_{n \to \infty}\paren{ \frac{\sup_{s \in[0,1]}{\abs{B_{n+s}-B_n}}}{n}}+\limsup_{n \to \infty}{\frac{\abs{B_n}}{n}} \notag\\
&=\limsup_{n \to \infty}\paren{ \frac{\sup_{s \in[0,1]}{\abs{B_{n+s}-B_n}}}{n}} < C \quad (c.s)
\end{align}
Entonces para toda $C > 0$ se tiene que:
$$
0 \leq \limsup_{t\to\infty }\frac{\abs{B_t}}{t}  \leq C 
$$
De dondo se concluye que $\lim_{t\to\infty }\frac{\abs{B_t}}{t} =0$
\end{proof}

\end{enumerate}
\end{problema}
\bibliography{GenBib}
\bibliographystyle{amsalpha}
\end{document}